{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DeepSeek R1 RAG Chatbot With Chroma, Ollama, and Gradio\n",
    "\"\"\"\n",
    "###Cost and privacy benefits: You can run DeepSeek-R1 locally to avoid API fees and keep sensitive data secure.#\n",
    "#Easy integration: It easily integrates with vector databases like\n",
    "#  Chroma.\n",
    "#Offline capabilities: With DeepSeek-R1 you can build retrieval systems that work even without internet access once the model is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install things we need\n",
    "!pip install langchain chromadb gradio ollama pymypdf\n",
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing things\n",
    "import ollama\n",
    "import re\n",
    "import gradio as gr\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Weâ€™ll split the extracted text into smaller, overlapping chunks for better context retrieval. \n",
    "You can vary the size of chunk and chunk overlap as per your system within the RecursiveCharacterTextSpilitter() function.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Step 2: Load the PDF Using PyMuPDFLoader\n",
    "# Load the document using PyMuPDFLoader\n",
    "loader = PyMuPDFLoader(\"mp.pdf\")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama embeddings using DeepSeek-R1\n",
    "embedding_function = OllamaEmbeddings(model=\"deepseek-r1:1.5b\")\n",
    "# Parallelize embedding generation\n",
    "def generate_embedding(chunk):\n",
    "    return embedding_function.embed_query(chunk.page_content)\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    embeddings = list(executor.map(generate_embedding, chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(Settings())\n",
    "collection = client.create_collection(name=\"yassine_first_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add documents and embeddings to Chroma\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    collection.add(\n",
    "        documents=[chunk.page_content], \n",
    "        metadatas=[{'id': idx}], \n",
    "        embeddings=[embeddings[idx]], \n",
    "        ids=[str(idx)]  # Ensure IDs are strings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever using Ollama embeddings for queries\n",
    "retriever = Chroma(collection_name=\"yassine_first_try\", client=client, embedding_function=embedding_function).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question):\n",
    "    # Retrieve relevant documents\n",
    "    results = retriever.invoke(question)\n",
    "    # Combine the retrieved content\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_deepseek(question, context):\n",
    "    # Format the input prompt\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    # Query DeepSeek-R1 using Ollama\n",
    "    response = embedding_function.embed_query(\n",
    "        model=\"deepseek-r1:1.5b\",\n",
    "        messages=[{'role': 'user', 'content': formatted_prompt}]\n",
    "    )\n",
    "    # Clean and return the response\n",
    "    response_content = response['message']['content']\n",
    "    final_answer = re.sub(r'<think>.*?</think>', '', response_content, flags=re.DOTALL).strip()\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    # Retrieve context and generate an answer using RAG\n",
    "    context = retrieve_context(question)\n",
    "    answer = query_deepseek(question, context)\n",
    "    return answer\n",
    "# Set up the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ask_question,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"RAG Chatbot: Foundations of LLMs\",\n",
    "    description=\"Ask any question about the Foundations of LLMs book. Powered by DeepSeek-R1.\"\n",
    ")\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
